{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 478750579/478750579 [00:20<00:00, 23263069.47B/s]\n",
      "100%|██████████| 656/656 [00:00<00:00, 453064.95B/s]\n",
      "100%|██████████| 815973/815973 [00:00<00:00, 1273602.56B/s]\n",
      "100%|██████████| 458495/458495 [00:00<00:00, 829414.60B/s]\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 specials tokens are used:\n",
    "- `<bos>` - beginning of sequence\n",
    "- `<eos>` - end of sequence\n",
    "- `<user>` - mark all the words uttered by user\n",
    "- `<bot>` - mark all the words uttered by bot\n",
    "- `<pad>` - pad token to make all the utterances equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<user>\", \"<bot>\", \"<pad>\"]\n",
    "\n",
    "#Adding tokens to the vocabulary\n",
    "tokenizer.set_special_tokens(SPECIAL_TOKENS)\n",
    "model.set_num_special_tokens(len(SPECIAL_TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# chain is used to iterate multidimensional list in element wise order without worrying about the indexes.\n",
    "\n",
    "# Let's define our contexts and special tokens\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "bos, eos, user, bot = \"<bos>\", \"<eos>\", \"<user>\", \"<bot>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_print(sequence):\n",
    "    for s in sequence:\n",
    "        print(s)\n",
    "        \n",
    "def build_inputs(persona, history, reply):\n",
    "    # create one single input sequence by concatenation and adding delimiters\n",
    "    # one list for persona, n-list of n-utterance in history, one list for reply\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    '''\n",
    "    ['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.']\n",
    "    ['hello', 'how', 'are', 'you', '?']\n",
    "    ['i', 'am', 'fine', 'thanks', '.']\n",
    "    ['great', 'to', 'hear', '<eos>']\n",
    "    '''\n",
    "    #sample_print(sequence)\n",
    "    # adding <user> and <bot> token, assuming history starts from user and bot and user speaks alternatively \n",
    "    # Also reply is just continuation of history\n",
    "    sequence = [sequence[0]] + [[user if i%2 else bot] + sequence[i] for i in range(1, len(sequence))]\n",
    "    '''\n",
    "    ['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.']\n",
    "    ['<user>', 'hello', 'how', 'are', 'you', '?']\n",
    "    ['<bot>', 'i', 'am', 'fine', 'thanks', '.']\n",
    "    ['<user>', 'great', 'to', 'hear', '<eos>']\n",
    "    '''\n",
    "    #sample_print(sequence)\n",
    "    \n",
    "    #build word, segments and positions token from sequence\n",
    "    words = list(chain(*sequence))\n",
    "    #persona is defined for bot so it belongs to bot segments\n",
    "    segments = [user if i%2 else bot for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))\n",
    "    \n",
    "    '''\n",
    "    words =    ['<bos>', 'i',     'like',  'playing', 'football', '.',     'i',     'am',    'from',  'NYC',   '.',     '<user>', 'hello',  'how',    'are',    'you',    '?',      '<bot>', 'i',     'am',    'fine',  'thanks', '.',     '<user>', 'great',  'to',     'hear',   '<eos>']\n",
    "    segments = ['<bot>', '<bot>', '<bot>', '<bot>',   '<bot>',    '<bot>', '<bot>', '<bot>', '<bot>', '<bot>', '<bot>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<bot>', '<bot>', '<bot>', '<bot>', '<bot>',  '<bot>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
    "    position = [0,        1,       2,       3,         4,          5,       6,       7,       8,       9,      10,       11,       12,       13,       14,       15,       16,       17,      18,      19,      20,      21,       22,      23,       24,       25,       26,       27]\n",
    "    '''\n",
    "    \n",
    "    return words, segments, position, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, segments, position, sequence = build_inputs(persona, history, reply)\n",
    "\n",
    "#As words need to converted to numebers for processing, tokenizer inbuilt function is used to assign id to each word\n",
    "words = tokenizer.convert_tokens_to_ids(words)\n",
    "segments = tokenizer.convert_tokens_to_ids(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40478, 11, 14594, 0, 0, 1, 11, 1574, 0, 0, 1, 40480, 0, 1991, 2183, 7159, 19, 40481, 11, 1574, 0, 12389, 1, 40480, 5201, 571, 863, 40479]\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40481, 40481, 40481, 40481, 40481, 40481, 40481, 40481, 40481, 40481, 40481, 40480, 40480, 40480, 40480, 40480, 40480, 40481, 40481, 40481, 40481, 40481, 40481, 40480, 40480, 40480, 40480, 40480]\n"
     ]
    }
   ],
   "source": [
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
